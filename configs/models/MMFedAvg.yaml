embedding_size: 512
latent_size: 64
feat_embed_dim: 64

is_federated: True
is_multimodal_model: True


fusion_module: 'moe' # 'moe', 'sum', 'attention', 'mlp', 'gate'
num_heads: 8 # number of heads in the attention module

learner: 'SGD'

clip_grad_norm: 1.0

use_neg_sampling: True

hyper_parameters: []

lr: 1e-1
l2_reg: 1e-8
