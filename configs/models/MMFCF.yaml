embedding_size: 512
latent_size: 64
feat_embed_dim: 64

is_federated: True
is_multimodal_model: True


fusion_module: 'moe' # 'moe', 'sum', 'attention', 'mlp', 'gate'
num_heads: 8 # number of heads in the attention module

learner: 'SGD'

use_neg_sampling: True

hyper_parameters: []

#lr: [ 1e-1,1e-2,1e-3,1e-4 ]
#l2_reg: [ 1e-4,1e-5,1e-6,1e-7,1e-8 ]

lr: 1e-1
l2_reg: 1e-4

