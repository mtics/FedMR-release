######################################################
#                       Method                       #
######################################################
model: "FedMAE"
is_federated: False


######################################################
#                      Dataset                       #
######################################################
data_path: "./datasets/"
dataset: "movielens"
data_file: "ml-100k.dat"
num_negative: 4
split: "predefined" # "holdout", "leave_one_out", "predefined"
holdout_rates: 0.1
data_type: "implicit"
use_negative_sampling: True
use_full_sampling: False

### For the case of multi-modal data
inter_splitting_label: "split_label"
filter_out_cod_start_users: True
is_multimodal_model: True

NEG_PREFIX: neg__
USER_ID_FIELD: user_id:token
ITEM_ID_FIELD: item_id:token
TIME_FIELD: timestamp:float
field_separator: "\t"

use_raw_features: False
max_txt_len: 32
max_img_size: 256
vocab_size: 30522
type_vocab_size: 2

hidden_size: 4
pad_token_id: 0
max_position_embeddings: 512
layer_norm_eps: 1e-12
hidden_dropout_prob: 0.1

######################################################
#           Evaluation & Client Sampling             #
######################################################
clients_sample_ratio: 1
clients_sample_strategy: "random"
top_k: 10
metrics: [ "Recall", "NDCG" ]
valid_metric: Recall@50
eval_batch_size: 4096


######################################################
#                     Training                       #
######################################################
early_stop: True
num_iterations: 100
local_epochs: 5
batch_size: 2048
tol: 1e-4
seed: 0
use_gpu: True
hardware: "gpu"
gpu_id: 0
require_training: True
optimizer: "adam"
save_model: False
paths:
  log: 'outputs/logs/{}/{}/'
  checkpoint: 'outputs/checkpoints/{}/{}/'
  save: 'outputs/results/{}/{}/{}/'

######################################################
#                  Hyperparameters                   #
######################################################
beta: 0.8
gamma: 0.1
lr: 1e-2
lr_scheduler: [ 1.0, 50 ]
l2_reg: 1e-8
decay_rate: 0.9
latent_size: 256
num_layers: 3
num_heads: 8
dropout: 0.5
q_dims: null
affine_type: "mlp"
anneal_cap: 0.2
noise_scale: 0.1
weight: 0.25

######################################################
#                        Others                      #
######################################################
type: "other"
comment: "test0"
on_server: False
notice: False
end2end: False

# iteration parameters
hyper_parameters:

######################################################
#                        MMRec                       #
######################################################

checkpoint_dir: 'saved'
save_recommended_topk: True
recommend_topk: 'recommend_topk/'

embedding_size: 64
weight_decay: 0.0
req_training: True
#embedding_size: 3780

# training settings
epochs: 100
stopping_step: 20
train_batch_size: 2048
learner: adam
learning_rate: 0.001
learning_rate_scheduler: [1.0, 50]
eval_step: 1

training_neg_sample_num: 4
use_neg_sampling: True

# evaluation settings
topk: [10, 20, 50]
